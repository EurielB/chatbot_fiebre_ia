# üå°Ô∏è Agente sobre Fiebre - Gu√≠a de Instalaci√≥n y Uso

Aplicaci√≥n web de chat especializada en informaci√≥n sobre fiebre, desarrollada con React + Vite y conectada a LM Studio u Ollama para procesamiento de lenguaje natural.

**‚ö†Ô∏è Nota:** Este es un prototipo para pruebas locales. Solo funciona en desarrollo.

## üìã Tabla de Contenidos

1. [Requisitos Previos](#requisitos-previos)
2. [Instalaci√≥n](#instalaci√≥n)
3. [Configuraci√≥n](#configuraci√≥n)
4. [Desarrollo Local](#desarrollo-local)
5. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## üîß Requisitos Previos

### Software Necesario

1. **Node.js** (versi√≥n 16 o superior)
   - Descargar desde: https://nodejs.org/
   - Verificar instalaci√≥n: `node --version`
   - Verificar npm: `npm --version`

2. **LM Studio** (recomendado) o **Ollama**
   - **LM Studio**: https://lmstudio.ai/
   - **Ollama**: https://ollama.ai/
   - Ver instrucciones detalladas en `INSTRUCCIONES_LM_STUDIO.md`

3. **Git** (opcional, para clonar el repositorio)
   - Descargar desde: https://git-scm.com/

### Sistema Operativo

- ‚úÖ Windows 10/11
- ‚úÖ macOS
- ‚úÖ Linux

---

## üì¶ Instalaci√≥n

### Paso 1: Obtener el C√≥digo

Si tienes el c√≥digo en un repositorio:
```bash
git clone <url-del-repositorio>
cd fiebre
```

Si tienes el c√≥digo en una carpeta, simplemente navega a ella:
```bash
cd ruta/a/fiebre
```

### Paso 2: Instalar Dependencias

Ejecuta el siguiente comando para instalar todas las dependencias necesarias:

```bash
npm install
```

Este comando instalar√°:
- React 18.3.1
- React DOM 18.3.1
- Vite 5.4.10
- TypeScript 5.6.3
- Y todas las dependencias de desarrollo necesarias

**Tiempo estimado:** 2-5 minutos (dependiendo de la velocidad de internet)

### Paso 3: Verificar Instalaci√≥n

Verifica que todo se instal√≥ correctamente:

```bash
npm list --depth=0
```

Deber√≠as ver las dependencias listadas sin errores.

---

## ‚öôÔ∏è Configuraci√≥n

### Configurar el Backend (LM Studio u Ollama)

#### Opci√≥n A: Usar LM Studio (Recomendado)

1. **Instalar LM Studio**
   - Descarga desde: https://lmstudio.ai/
   - Instala y ejecuta la aplicaci√≥n

2. **Descargar el Modelo**
   - En LM Studio, ve a la pesta√±a **"Search"**
   - Busca y descarga: `google/gemma-3-1b`
   - Espera a que termine la descarga

3. **Cargar el Modelo** ‚ö†Ô∏è **IMPORTANTE**
   - Ve a la pesta√±a **"Chat"** o **"Local Server"**
   - Selecciona el modelo descargado en el selector superior
   - Espera a que aparezca "Loaded" o similar

4. **Iniciar el Servidor**
   - Ve a la pesta√±a **"Local Server"**
   - Haz clic en **"Start Server"**
   - Deber√≠a mostrar: "Server running on http://localhost:1234"

5. **Configurar la Aplicaci√≥n**
   - Abre `src/components/Chat.tsx`
   - Verifica que la l√≠nea 10 tenga:
     ```typescript
     const BACKEND_TYPE: 'ollama' | 'lmstudio' = 'lmstudio'
     ```
   - Verifica que la l√≠nea 11 tenga el nombre correcto del modelo:
     ```typescript
     const MODEL_NAME = 'google/gemma-3-1b'
     ```

#### Opci√≥n B: Usar Ollama

1. **Instalar Ollama**
   - Descargar desde: https://ollama.ai/
   - Instalar y ejecutar

2. **Descargar el Modelo**
   ```bash
   ollama pull google/gemma-3-1b
   ```
   Espera a que termine la descarga del modelo.

3. **Verificar que Ollama est√© corriendo**
   - Ollama deber√≠a iniciarse autom√°ticamente
   - Verifica en: http://localhost:11434

4. **Configurar la Aplicaci√≥n**
   - Abre `src/components/Chat.tsx`
   - Cambia la l√≠nea 10 a:
     ```typescript
     const BACKEND_TYPE: 'ollama' | 'lmstudio' = 'ollama'
     ```
   - Verifica que la l√≠nea 11 tenga:
     ```typescript
     const MODEL_NAME = 'google/gemma-3-1b'
     ```

---

## üöÄ Desarrollo Local

### Paso 1: Iniciar el Servidor de Desarrollo

Abre una terminal en la carpeta del proyecto y ejecuta:

```bash
npm run dev
```

Deber√≠as ver algo como:
```
  VITE v5.4.10  ready in 500 ms

  ‚ûú  Local:   http://localhost:5173/
  ‚ûú  Network: use --host to expose
```

### Paso 2: Abrir en el Navegador

Abre tu navegador y ve a:
```
http://localhost:5173
```

### Paso 3: Verificar la Conexi√≥n

1. En la aplicaci√≥n, haz clic en el bot√≥n **"üîå Probar conexi√≥n"**
2. Deber√≠as ver: "Conexi√≥n OK: X modelo(s) detectados"
3. Si hay error, consulta la secci√≥n [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

### Paso 4: Probar la Aplicaci√≥n

1. Escribe una pregunta en el campo de texto, por ejemplo: "¬øQu√© es la fiebre?"
2. Haz clic en **"üì§ Enviar"** o presiona Enter
3. Deber√≠as recibir una respuesta del asistente

---

## üîç Soluci√≥n de Problemas

### Error: "npm: command not found"

**Soluci√≥n:** Node.js no est√° instalado o no est√° en el PATH. Instala Node.js desde https://nodejs.org/

### Error: "Cannot find module"

**Soluci√≥n:** Las dependencias no est√°n instaladas. Ejecuta:
```bash
npm install
```

### Error al ejecutar `npm run dev`

**Soluci√≥n 1:** Verifica que el puerto 5173 no est√© en uso:
```bash
# Windows
netstat -ano | findstr :5173

# Linux/Mac
lsof -i :5173
```

**Soluci√≥n 2:** Cambia el puerto en `vite.config.ts`:
```typescript
server: {
  port: 3000, // Cambia a otro puerto
}
```

### Error: "Conexi√≥n fallida" al probar la conexi√≥n

**Causas y Soluciones:**

1. **LM Studio no est√° corriendo**
   - Abre LM Studio
   - Ve a "Local Server"
   - Haz clic en "Start Server"
   - Verifica que diga "Server running on http://localhost:1234"

2. **El modelo no est√° cargado**
   - En LM Studio, ve a la pesta√±a "Chat"
   - Selecciona el modelo en el selector superior
   - Espera a que aparezca "Loaded"
   - Luego inicia el servidor en "Local Server"

3. **Puerto incorrecto**
   - Verifica que LM Studio est√© en el puerto 1234
   - O que Ollama est√© en el puerto 11434
   - Verifica la configuraci√≥n en `vite.config.ts`

4. **Firewall bloqueando**
   - Permite Node.js y LM Studio/Ollama en el firewall de Windows

### Error: "model_not_found" o "No models loaded"

**Soluci√≥n:**
- El modelo debe estar **CARGADO** antes de usar el servidor
- En LM Studio: Ve a "Chat" ‚Üí Selecciona el modelo ‚Üí Espera a que cargue
- Luego inicia el servidor en "Local Server"

### Error: El modelo no responde

**Soluciones:**

1. Verifica que el nombre del modelo en `Chat.tsx` coincida exactamente con el nombre en LM Studio/Ollama

2. Verifica que el modelo `google/gemma-3-1b` est√© correctamente descargado y cargado

3. Revisa la consola del navegador (F12) para ver errores detallados

4. Verifica que el modelo est√© completamente cargado

### Error en TypeScript: "compilation failed"

**Soluci√≥n:**
1. Verifica que todos los archivos TypeScript est√©n correctos:
   ```bash
   npx tsc --noEmit
   ```

2. Corrige los errores mostrados

---

## üìù Comandos √ötiles

```bash
# Instalar dependencias
npm install

# Desarrollo local
npm run dev

# Verificar tipos TypeScript
npx tsc --noEmit

# Limpiar node_modules (si hay problemas)
# Windows
rmdir /s /q node_modules
del package-lock.json
npm install

# Linux/Mac
rm -rf node_modules package-lock.json
npm install
```

---

## üìö Estructura del Proyecto

```
fiebre/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Chat.tsx          # Componente principal del chat
‚îÇ   ‚îú‚îÄ‚îÄ App.tsx               # Componente principal de la app
‚îÇ   ‚îú‚îÄ‚îÄ main.tsx              # Punto de entrada
‚îÇ   ‚îî‚îÄ‚îÄ styles.css            # Estilos globales
‚îú‚îÄ‚îÄ node_modules/             # Dependencias (generadas con npm install)
‚îú‚îÄ‚îÄ index.html                # HTML principal
‚îú‚îÄ‚îÄ package.json              # Configuraci√≥n y dependencias
‚îú‚îÄ‚îÄ vite.config.ts            # Configuraci√≥n de Vite (incluye proxy para desarrollo)
‚îú‚îÄ‚îÄ tsconfig.json             # Configuraci√≥n de TypeScript
‚îú‚îÄ‚îÄ backend_server.py         # Servidor Python alternativo (opcional)
‚îú‚îÄ‚îÄ INSTRUCCIONES_LM_STUDIO.md # Instrucciones detalladas de LM Studio
‚îî‚îÄ‚îÄ README.md                 # Este archivo
```

---

## üÜò Soporte

Si encuentras problemas:

1. Revisa la secci√≥n [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
2. Verifica que todos los requisitos previos est√©n instalados
3. Revisa los logs de la consola del navegador (F12)
4. Revisa los logs del servidor de desarrollo
5. Consulta `INSTRUCCIONES_LM_STUDIO.md` para problemas espec√≠ficos de LM Studio

---

## üìÑ Licencia

Este proyecto es de uso educativo y no sustituye atenci√≥n m√©dica profesional.

---

**¬°Listo para probar!** üöÄ

Este es un prototipo que funciona solo en desarrollo local. Aseg√∫rate de tener LM Studio u Ollama corriendo antes de usar la aplicaci√≥n.

